{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Causal Effects of Actors on Movie Revenue\n",
    "An example in Model Based Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Bernoulli\n",
    "from pyro.distributions import Delta\n",
    "from pyro.distributions import Normal\n",
    "from pyro.distributions import Uniform\n",
    "from pyro.distributions import LogNormal\n",
    "from pyro.infer import SVI\n",
    "from pyro.infer import Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "import torch.distributions.constraints as constraints\n",
    "pyro.set_rng_seed(101)\n",
    "\n",
    "# Data loader\n",
    "sys.path.insert(1, '../')\n",
    "from box_office.data_loader import load_tensor_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "Lets say producers of a movie approaches you with a set of actors they'd like to cast in their movie and want you to find out 2 things: <br>\n",
    "    1) How much box office revenue will this movie make with this cast of actors?<br>\n",
    "    2) How much of that revenue will every actor be responsible for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"Data Science\" Solution\n",
    "\n",
    "You scrape IMDB, get a list of movies that have grossed 1 million or higher (that's the least a decent movie could do), and retain actors who have appeared in at least 10 movies. Then arrange it in a data frame where every movie is a row, actors are columns, and presence or absence of actors in that movie is denoted as 1 or 0. <br>\n",
    "That looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data from the dataframe\n",
    "data_loc = \"../data/ohe_movies.csv\"\n",
    "x_train_tensors, y_train_tensors, actors, og_movies = load_tensor_data(data_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "og_movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit a Linear Regression to this dataset, or any Linear Model for that matter even a Neural Network. We'll go with Linear Regression for now because regression estimates directly represent a proportional relation with the outcome variable i.e we can treat them as causal effects. <br>\n",
    "This seems like a good idea till we inspect what happens to the regression coefficients. <br>\n",
    "Fitting a Linear Model hides confounders that affect both actors and revenues. For example, genre is a confounder. Take Action and comedy. Action movies on average make more than Comedy movies and each tend to cast a different set of actors.<br> \n",
    "When unobserved the genre produces statistical dependence between if an actor is in the movie and its revenue.\n",
    "So the causal estimates for every actor are biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judi Dench played M in every James Bond movie from 1995 to 2012.\n",
    "Cobie Smulders plays Maria Hill in every Avenger’s movies.\n",
    "These are 2 well known but not massively popular actors who appear in high grossing movies.\n",
    "The regression estimates for them are biased, and will lead to an overestimate of revenue for a new movie casting them. This is because our DAG looks like this: \n",
    "![DAG2](figs/ml_dag.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Causal Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We argue that there are certain common factors that go into picking a cast for a movie and the revenue that the movie generates. In Causality Theory these are called *Confounders*. So we propose the following data generation process: where certain unknown confounders *Z* influence the set of actors *A* and the revenue *R*. This is represented as a Bayesian Network.\n",
    "![DAG](../figs/causal_dag.jpg)\n",
    "\n",
    "We will now stick with this generative model and figure a way to unbias our causal regression estimates. If we somehow find a way to estimate Z, then we can include it in our Regression Model and obtain unbiased causal estimates as regression coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how every individual function for a variable will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_z(params):\n",
    "    \"\"\"Samples from P(Z)\"\"\"    \n",
    "    z_mean0 = params['z_mean0']\n",
    "    z_std0 = params['z_std0']\n",
    "    z = pyro.sample(\"z\", Normal(loc = z_mean0, scale = z_std0))\n",
    "    return z\n",
    "\n",
    "def f_x(z, params):\n",
    "    \"\"\"\n",
    "    Samples from P(X|Z)\n",
    "    \n",
    "    P(X|Z) is a Bernoulli with E(X|Z) = logistic(Z * W),\n",
    "    where W is a parameter (matrix).  In training the W is\n",
    "    hyperparameters of the W distribution are estimated such\n",
    "    that in P(X|Z), the elements of the vector of X are\n",
    "    conditionally independent of one another given Z.\n",
    "    \"\"\"\n",
    "    def sample_W():\n",
    "        \"\"\"\n",
    "        Sample the W matrix\n",
    "        \n",
    "        W is a parameter of P(X|Z) that is sampled from a Normal\n",
    "        with location and scale hyperparameters w_mean0 and w_std0\n",
    "        \"\"\"\n",
    "        w_mean0 = params['w_mean0']\n",
    "        w_std0 = params['w_std0']\n",
    "        W = pyro.sample(\"W\", Normal(loc = w_mean0, scale = w_std0))\n",
    "        return W\n",
    "    W = sample_W()\n",
    "    linear_exp = torch.matmul(z, W)\n",
    "    # sample x using the Bernoulli likelihood\n",
    "    x = pyro.sample(\"x\", Bernoulli(logits = linear_exp))\n",
    "    return x\n",
    "\n",
    "def f_y(x, z, params):\n",
    "    \"\"\"\n",
    "    Samples from P(Y|X, Z)\n",
    "    \n",
    "    Y is sampled from a Gaussian where the mean is an\n",
    "    affine combination of X and Z.  Bayesian linear\n",
    "    regression is used to estimate the parameters of\n",
    "    this affine transformation  function.  Use torch.nn.Module to create\n",
    "    the Bayesian linear regression component of the overall\n",
    "    model.\n",
    "    \"\"\"\n",
    "    predictors = torch.cat((x, z), 1)\n",
    "\n",
    "    w = pyro.sample('weight', Normal(params['weight_mean0'], params['weight_std0']))\n",
    "    b = pyro.sample('bias', Normal(params['bias_mean0'], params['bias_std0']))\n",
    "\n",
    "    y_hat = (w * predictors).sum(dim=1) + b\n",
    "    # variance of distribution centered around y\n",
    "    sigma = pyro.sample('sigma', Normal(params['sigma_mean0'], params['sigma_std0']))\n",
    "    with pyro.iarange('data', len(predictors)):\n",
    "        pyro.sample('y', LogNormal(y_hat, sigma))\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is our complete generative causal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(params):\n",
    "    \"\"\"The full generative causal model\"\"\"\n",
    "    z = f_z(params)\n",
    "    x = f_x(z, params)\n",
    "    y = f_y(x, z, params)\n",
    "    return {'z': z, 'x': x, 'y': y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to infer Z?\n",
    "\n",
    "If we could somehow measure all confounders that affect choice of cast and revenue generated by that cast, then we could  condition on them and obtain unbiased estimates. This is the Ignorability assumption: that the outcome is independent of treatment assignment (choice of actors), so now average difference in outcomes between two groups of actors can only be attributable to the treatment (actors). The problem here is that it's impossible to check if we've measured all confounders. So Yixin Wang and David M. Blei proposed an algorithm; “The Deconfounder” to sidestep the search for confounders because its impossible to exhaust them.\n",
    "They find a latent variable model over the causes.\n",
    "Use it to infer latent variables for each individual movie.\n",
    "Then use this inferred variable as a “substitute confounder” and get back to treating this as a regression problem with the inferred variables as more data. So we use a probabilistic PCA model over actors to infer the latent variables that explain the distribution of actors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_1_guide(params):\n",
    "    \"\"\"\n",
    "    Guide function for fitting P(Z) and P(X|Z) from data\n",
    "    \"\"\"\n",
    "    # Infer z hyperparams\n",
    "    qz_mean = pyro.param(\"qz_mean\", params['z_mean0'])\n",
    "    qz_stddv = pyro.param(\"qz_stddv\", params['z_std0'],\n",
    "                         constraint=constraints.positive)\n",
    "    \n",
    "    z = pyro.sample(\"z\", Normal(loc = qz_mean, scale = qz_stddv))\n",
    "    \n",
    "    # Infer w params\n",
    "    qw_mean = pyro.param(\"qw_mean\", params[\"w_mean0\"])\n",
    "    qw_stddv = pyro.param(\"qw_stddv\", params[\"w_std0\"],\n",
    "                          constraint=constraints.positive)\n",
    "    W = pyro.sample(\"W\", Normal(loc = qw_mean, scale = qw_stddv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Pyro's _\"guide\"_ functionality to infer $P(Z)$ and $P(X|Z)$ using Stochastic Variational Inference, a scalable algorithm for approximating posterior distributions. For this, we define the above guide function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary goal however, is to estimate the causal estimates of actors which are the linear regression coefficients for each actor. For this we will write another guide function: one that optimizes for the linear regression parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_2_guide(params):\n",
    "    # Z and W are just sampled using param values optimized in previous step\n",
    "    z = pyro.sample(\"z\", Normal(loc = params['qz_mean'], scale = params['qz_stddv']))\n",
    "    W = pyro.sample(\"W\", Normal(loc = params['qw_mean'], scale = params['qw_stddv']))\n",
    "    \n",
    "    # Infer regression params\n",
    "    # parameters of (w : weight)\n",
    "    w_loc = pyro.param('w_loc', params['weight_mean0'])\n",
    "    w_scale = pyro.param('w_scale', params['weight_std0'])\n",
    "\n",
    "    # parameters of (b : bias)\n",
    "    b_loc = pyro.param('b_loc', params['bias_mean0'])\n",
    "    b_scale = pyro.param('b_scale', params['bias_std0'])\n",
    "    # parameters of (sigma)\n",
    "    sigma_loc = pyro.param('sigma_loc', params['sigma_mean0'])\n",
    "    sigma_scale = pyro.param('sigma_scale', params['sigma_std0'])\n",
    "\n",
    "    # sample (w, b, sigma)\n",
    "    w = pyro.sample('weight', Normal(w_loc, w_scale))\n",
    "    b = pyro.sample('bias', Normal(b_loc, b_scale))\n",
    "    sigma = pyro.sample('sigma', Normal(sigma_loc, sigma_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary difference, between what Wang and Blei have done, and what we will do here, is that we have implemented our beliefs as a generative model. The factor model (Probabilistic PCA) and the regression have been combined into one model over the DAG. <br>\n",
    "It's important to understand that Wang et. al. separate the estimation of 𝑍 from the estimation of regression parameters.\n",
    "This is done because 𝑍 by construction renders all causes (actors) independent of each other. \n",
    "Including the outcome (revenue) while learning parameters of 𝑍 would make the revenue conditionally independent of the actors which violates our primary assumption that actors are a cause of movie revenue.\n",
    "So they estimate 𝑍, then hardcode it into their regression problem.<br>\n",
    "<br>\n",
    "We handle this by running a 2 step training process in Pyro. That is with two different guide functions over the same DAG to optimize different parameters conditional on certain variables at a time. <br>\n",
    "The first learns posterior of 𝑍 and 𝑊(a parameter of P(X|Z)) conditional on 𝑋. <br>\n",
    "The second learns the regression parameters conditional on what we what we know about 𝑋, what we just learnt about 𝑊, and what we know about 𝑌. <br>\n",
    "Once they are defined, we only need to train this generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_step_1(x_data, params):\n",
    "    \n",
    "    adam_params = {\"lr\": 0.0005}\n",
    "    optimizer = Adam(adam_params)\n",
    "\n",
    "    conditioned_on_x = pyro.condition(model, data = {\"x\" : x_data})\n",
    "    svi = SVI(conditioned_on_x, step_1_guide, optimizer, loss=Trace_ELBO())\n",
    "    \n",
    "    print(\"\\n Training Z marginal and W parameter marginal...\")\n",
    "\n",
    "    n_steps = 2000\n",
    "    pyro.set_rng_seed(101)\n",
    "    # do gradient steps\n",
    "    pyro.get_param_store().clear()\n",
    "    for step in range(n_steps):\n",
    "        loss = svi.step(params)\n",
    "        if step % 100 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (step + 1, loss/len(x_data)))\n",
    "            \n",
    "    # grab the learned variational parameters\n",
    "    \n",
    "    updated_params = {k: v for k, v in params.items()}\n",
    "    for name, value in pyro.get_param_store().items():\n",
    "        print(\"Updating value of hypermeter{}\".format(name))\n",
    "        updated_params[name] = value.detach()\n",
    "        \n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_step_2(x_data, y_data, params):\n",
    "    print(\"Training Bayesian regression parameters...\")\n",
    "    pyro.set_rng_seed(101)\n",
    "    num_iterations = 1500\n",
    "    pyro.clear_param_store()\n",
    "    # Create a regression model\n",
    "    optim = Adam({\"lr\": 0.003})\n",
    "    conditioned_on_x_and_y = pyro.condition(model, data = {\n",
    "        \"x\": x_data,\n",
    "        \"y\" : y_data\n",
    "    })\n",
    "\n",
    "    svi = SVI(conditioned_on_x_and_y, step_2_guide, optim, loss=Trace_ELBO(), num_samples=1000)\n",
    "    for step in range(num_iterations):\n",
    "        loss = svi.step(params)\n",
    "        if step % 100 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (step + 1, loss/len(x_data)))\n",
    "    \n",
    "    \n",
    "    updated_params = {k: v for k, v in params.items()}\n",
    "    for name, value in pyro.get_param_store().items():\n",
    "        print(\"Updating value of hypermeter: {}\".format(name))\n",
    "        updated_params[name] = value.detach()\n",
    "    print(\"Training complete.\")\n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    num_datapoints, data_dim = x_train_tensors.shape\n",
    "    \n",
    "    latent_dim = 30 # can be changed\n",
    "    params0 = {\n",
    "        'z_mean0': torch.zeros([num_datapoints, latent_dim]),\n",
    "        'z_std0' : torch.ones([num_datapoints, latent_dim]),\n",
    "        'w_mean0' : torch.zeros([latent_dim, data_dim]),\n",
    "        'w_std0' : torch.ones([latent_dim, data_dim]),\n",
    "        'weight_mean0': torch.zeros(data_dim + latent_dim),\n",
    "        'weight_std0': torch.ones(data_dim + latent_dim),\n",
    "        'bias_mean0': torch.tensor(0.),\n",
    "        'bias_std0': torch.tensor(1.),\n",
    "        'sigma_mean0' : torch.tensor(1.),\n",
    "        'sigma_std0' : torch.tensor(0.05)\n",
    "    } # These are our priors\n",
    "\n",
    "    params1 = training_step_1(x_train_tensors, params0)\n",
    "    params2 = training_step_2(x_train_tensors, y_train_tensors, params1)\n",
    "    return params1, params2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we train the model to infer latent variable distributions and Bayesian Regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p1, p2 = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal effect of actors with and without confounding\n",
    "Because we have implemented all our assumptions as one generative model, finding causal estimates of actors is as simple as calling the condition and do queries from Pyro.\n",
    "Causal effect of actors without confounding: $E[Y|X=1] - E[Y|X=0]$ <br>\n",
    "Causal effect of actors without confounding: $E[Y|do(X=1)] - E[Y|do(X=0)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def condCausal(their_tensors, absent_tensors, movie_inds):\n",
    "    their_cond = pyro.condition(model, data = {\"x\" : their_tensors})\n",
    "    absent_cond = pyro.condition(model, data = {\"x\" : absent_tensors})\n",
    "    \n",
    "    their_y = []\n",
    "    for _ in range(1000):\n",
    "        their_y.append(torch.sum(their_cond(p2)['y'][movie_inds]).item())\n",
    "    \n",
    "    absent_y = []\n",
    "    for _ in range(1000):\n",
    "        absent_y.append(torch.sum(absent_cond(p2)['y'][movie_inds]).item())\n",
    "    \n",
    "    their_mean = np.mean(their_y)\n",
    "    absent_mean = np.mean(absent_y)\n",
    "    causal_effect_noconf = their_mean - absent_mean\n",
    "    \n",
    "    return causal_effect_noconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doCausal(their_tensors, absent_tensors, movie_inds):\n",
    "    # With confounding\n",
    "    their_do = pyro.do(model, data = {\"x\" : their_tensors})\n",
    "    absent_do = pyro.do(model, data = {\"x\" : absent_tensors})\n",
    "    \n",
    "    their_do_y = []\n",
    "    for _ in range(1000):\n",
    "        their_do_y.append(torch.sum(their_do(p2)['y'][movie_inds]).item())\n",
    "    \n",
    "    absent_do_y = []\n",
    "    for _ in range(1000):\n",
    "        absent_do_y.append(torch.sum(absent_do(p2)['y'][movie_inds]).item())\n",
    "    \n",
    "    their_do_mean = np.mean(their_do_y)\n",
    "    absent_do_mean = np.mean(absent_do_y)\n",
    "    causal_effect_conf = their_do_mean - absent_do_mean\n",
    "    \n",
    "    return causal_effect_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def causal_effects(actor):\n",
    "    # Get all movies where that actor is present\n",
    "    # Make him/her absent, and then get conditional expectation\n",
    "    \n",
    "    actor_tensor = pd.DataFrame(x_train_tensors.numpy(), columns=actors[1:])\n",
    "    # All movies where actor is present\n",
    "    movie_inds = actor_tensor.index[actor_tensor[actor] == 1.0]\n",
    "\n",
    "    absent_movies = actor_tensor.copy()\n",
    "    absent_movies[actor] = 0\n",
    "    \n",
    "    their_tensors = x_train_tensors\n",
    "    absent_tensors = torch.tensor(absent_movies.to_numpy(dtype = 'float32'))\n",
    "    \n",
    "    cond_effect_mean = condCausal(their_tensors, absent_tensors, movie_inds)\n",
    "    do_effect_mean = doCausal(their_tensors, absent_tensors, movie_inds)\n",
    "#     print(their_tensors.shape, absent_tensors.shape)\n",
    "    diff_mean = cond_effect_mean - do_effect_mean\n",
    "    if diff_mean > 0:\n",
    "        status = \"Overvalued\"\n",
    "    else: status = \"Undervalued\"\n",
    "    \n",
    "    print(\"Causal conditional effect: \", cond_effect_mean)\n",
    "    \n",
    "    print(\"Causal Do effect: \", do_effect_mean)\n",
    "    \n",
    "    print(\"Diff: \", diff_mean)\n",
    "    \n",
    "    print(\"Status: \", status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call these queries on the actors for whome we'd like to see biased and unbiased estimates for. Cobie Smulders and Judi Dench are the examples we set out to prove our point with, and the our generative model does obtain debiased estimates of their causal effect on revenue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "causal_effects(\"Cobie Smulders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "causal_effects(\"Judi Dench\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal effect of Actor $j$ on Revenue without conditioning for the confounders can be obtained by $E[Y|X_j=1]−E[Y|X_j=0]$\n",
    "This retains the original DAG and samples at site $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal effect of Actor j on Revenue without conditioning for the confounders can be obtained by $E[Y|do(X_j=1)] - E[Y|do(X_j=0)]$\n",
    "This mutates the DAG as the *do* operator removes all incoming edges into a node. We then sample at site $Y$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
