{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Bernoulli\n",
    "from pyro.distributions import Delta\n",
    "from pyro.distributions import Normal\n",
    "from pyro.distributions import Uniform\n",
    "from pyro.distributions import LogNormal\n",
    "from pyro.infer import SVI\n",
    "from pyro.infer import Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "import torch.distributions.constraints as constraints\n",
    "\n",
    "# initialize the autodiagonal with init_to_feasible instead of init_to_median\n",
    "from pyro.infer.autoguide import init_to_feasible\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "# Data Loader\n",
    "import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../')\n",
    "from box_office.data_loader import load_tensor_data\n",
    "\n",
    "pyro.set_rng_seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_tensors, y_train_tensors, actors, full_data = load_tensor_data(\"../data/ohe_movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_z(params):\n",
    "    \"\"\"Samples from P(Z)\"\"\"    \n",
    "    z_mean0 = params['z_mean0']\n",
    "    z_std0 = params['z_std0']\n",
    "    z = pyro.sample(\"z\", Normal(loc = z_mean0, scale = z_std0))\n",
    "    return z\n",
    "\n",
    "def f_x(z, params):\n",
    "    \"\"\"\n",
    "    Samples from P(X|Z)\n",
    "    \n",
    "    P(X|Z) is a Bernoulli with E(X|Z) = logistic(Z * W),\n",
    "    where W is a parameter (matrix).  In training the W is\n",
    "    hyperparameters of the W distribution are estimated such\n",
    "    that in P(X|Z), the elements of the vector of X are\n",
    "    conditionally independent of one another given Z.\n",
    "    \"\"\"\n",
    "    def sample_W():\n",
    "        \"\"\"\n",
    "        Sample the W matrix\n",
    "        \n",
    "        W is a parameter of P(X|Z) that is sampled from a Normal\n",
    "        with location and scale hyperparameters w_mean0 and w_std0\n",
    "        \"\"\"\n",
    "        w_mean0 = params['w_mean0']\n",
    "        w_std0 = params['w_std0']\n",
    "        W = pyro.sample(\"W\", Normal(loc = w_mean0, scale = w_std0))\n",
    "        return W\n",
    "    W = sample_W()\n",
    "    linear_exp = torch.matmul(z, W)\n",
    "    # sample x using the Bernoulli likelihood\n",
    "    x = pyro.sample(\"x\", Bernoulli(logits = linear_exp))\n",
    "    return x\n",
    "\n",
    "def f_y(x, z, params):\n",
    "    \"\"\"\n",
    "    Samples from P(Y|X, Z)\n",
    "    \n",
    "    Y is sampled from a Gaussian where the mean is an\n",
    "    affine combination of X and Z.  Bayesian linear\n",
    "    regression is used to estimate the parameters of\n",
    "    this affine transformation  function.  Use torch.nn.Module to create\n",
    "    the Bayesian linear regression component of the overall\n",
    "    model.\n",
    "    \"\"\"\n",
    "    predictors = torch.cat((x, z), 1)\n",
    "\n",
    "    w = pyro.sample('weight', Normal(params['weight_mean0'], params['weight_std0']))\n",
    "    b = pyro.sample('bias', Normal(params['bias_mean0'], params['bias_std0']))\n",
    "\n",
    "    y_hat = (w * predictors).sum(dim=1) + b\n",
    "    # variance of distribution centered around y\n",
    "    sigma = pyro.sample('sigma', Normal(params['sigma_mean0'], params['sigma_std0']))\n",
    "    with pyro.iarange('data', len(predictors)):\n",
    "        pyro.sample('y', LogNormal(y_hat, sigma))\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(params):\n",
    "    \"\"\"The full generative causal model\"\"\"\n",
    "    z = f_z(params)\n",
    "    x = f_x(z, params)\n",
    "    y = f_y(x, z, params)\n",
    "    return {'z': z, 'x': x, 'y': y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_1_guide(params):\n",
    "    \"\"\"\n",
    "    Guide function for fitting P(Z) and P(X|Z) from data\n",
    "    \"\"\"\n",
    "    # Infer z hyperparams\n",
    "    qz_mean = pyro.param(\"qz_mean\", params['z_mean0'])\n",
    "    qz_stddv = pyro.param(\"qz_stddv\", params['z_std0'],\n",
    "                         constraint=constraints.positive)\n",
    "    \n",
    "    z = pyro.sample(\"z\", Normal(loc = qz_mean, scale = qz_stddv))\n",
    "    \n",
    "    # Infer w params\n",
    "    qw_mean = pyro.param(\"qw_mean\", params[\"w_mean0\"])\n",
    "    qw_stddv = pyro.param(\"qw_stddv\", params[\"w_std0\"],\n",
    "                          constraint=constraints.positive)\n",
    "    W = pyro.sample(\"W\", Normal(loc = qw_mean, scale = qw_stddv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_2_guide(params):\n",
    "    # Z and W are just sampled using param values optimized in previous step\n",
    "    z = pyro.sample(\"z\", Normal(loc = params['qz_mean'], scale = params['qz_stddv']))\n",
    "    W = pyro.sample(\"W\", Normal(loc = params['qw_mean'], scale = params['qw_stddv']))\n",
    "    \n",
    "    # Infer regression params\n",
    "    # parameters of (w : weight)\n",
    "    w_loc = pyro.param('w_loc', params['weight_mean0'])\n",
    "    w_scale = pyro.param('w_scale', params['weight_std0'])\n",
    "\n",
    "    # parameters of (b : bias)\n",
    "    b_loc = pyro.param('b_loc', params['bias_mean0'])\n",
    "    b_scale = pyro.param('b_scale', params['bias_std0'])\n",
    "    # parameters of (sigma)\n",
    "    sigma_loc = pyro.param('sigma_loc', params['sigma_mean0'])\n",
    "    sigma_scale = pyro.param('sigma_scale', params['sigma_std0'])\n",
    "\n",
    "    # sample (w, b, sigma)\n",
    "    w = pyro.sample('weight', Normal(w_loc, w_scale))\n",
    "    b = pyro.sample('bias', Normal(b_loc, b_scale))\n",
    "    sigma = pyro.sample('sigma', Normal(sigma_loc, sigma_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_step_1(x_data, params):\n",
    "    \n",
    "    adam_params = {\"lr\": 0.0005}\n",
    "    optimizer = Adam(adam_params)\n",
    "\n",
    "    conditioned_on_x = pyro.condition(model, data = {\"x\" : x_data})\n",
    "    svi = SVI(conditioned_on_x, step_1_guide, optimizer, loss=Trace_ELBO())\n",
    "    \n",
    "    print(\"\\n Training Z marginal and W parameter marginal...\")\n",
    "\n",
    "    n_steps = 2000\n",
    "    pyro.set_rng_seed(101)\n",
    "    # do gradient steps\n",
    "    pyro.get_param_store().clear()\n",
    "    for step in range(n_steps):\n",
    "        loss = svi.step(params)\n",
    "        if step % 100 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (step + 1, loss/len(x_data)))\n",
    "            \n",
    "    # grab the learned variational parameters\n",
    "    \n",
    "    updated_params = {k: v for k, v in params.items()}\n",
    "    for name, value in pyro.get_param_store().items():\n",
    "        print(\"Updating value of hypermeter{}\".format(name))\n",
    "        updated_params[name] = value.detach()\n",
    "        \n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_step_2(x_data, y_data, params):\n",
    "    print(\"Training Bayesian regression parameters...\")\n",
    "    pyro.set_rng_seed(101)\n",
    "    num_iterations = 1000\n",
    "    pyro.clear_param_store()\n",
    "    # Create a regression model\n",
    "    optim = Adam({\"lr\": 0.003})\n",
    "    conditioned_on_x_and_y = pyro.condition(model, data = {\n",
    "        \"x\": x_data,\n",
    "        \"y\" : y_data\n",
    "    })\n",
    "\n",
    "    svi = SVI(conditioned_on_x_and_y, step_2_guide, optim, loss=Trace_ELBO(), num_samples=1000)\n",
    "    for step in range(num_iterations):\n",
    "        loss = svi.step(params)\n",
    "        if step % 100 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (step + 1, loss/len(x_data)))\n",
    "    \n",
    "    \n",
    "    updated_params = {k: v for k, v in params.items()}\n",
    "    for name, value in pyro.get_param_store().items():\n",
    "        print(\"Updating value of hypermeter: {}\".format(name))\n",
    "        updated_params[name] = value.detach()\n",
    "    print(\"Training complete.\")\n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    num_datapoints, data_dim = x_train_tensors.shape\n",
    "    \n",
    "    latent_dim = 30 # can be changed\n",
    "#     print(torch.zeros(data_dim + latent_dim).shape)\n",
    "    params0 = {\n",
    "        'z_mean0': torch.zeros([num_datapoints, latent_dim]),\n",
    "        'z_std0' : torch.ones([num_datapoints, latent_dim]),\n",
    "        'w_mean0' : torch.zeros([latent_dim, data_dim]),\n",
    "        'w_std0' : torch.ones([latent_dim, data_dim]),\n",
    "        'weight_mean0': torch.zeros(data_dim + latent_dim),\n",
    "        'weight_std0': torch.ones(data_dim + latent_dim),\n",
    "        'bias_mean0': torch.tensor(0.),\n",
    "        'bias_std0': torch.tensor(1.),\n",
    "        'sigma_mean0' : torch.tensor(1.),\n",
    "        'sigma_std0' : torch.tensor(0.05)\n",
    "    }\n",
    "\n",
    "    params1 = training_step_1(x_train_tensors, params0)\n",
    "    params2 = training_step_2(x_train_tensors, y_train_tensors, params1)\n",
    "    return params1, params2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trained_params = train_model()\n",
    "p1, p2 = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save all params to disk\n",
    "import pickle\n",
    "\n",
    "with open('params.pickle', 'wb') as handle:\n",
    "    pickle.dump(p2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
